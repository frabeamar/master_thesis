% set counter to n-1:
\setcounter{chapter}{1}

\chapter{Related Work}



Sample references are~\cite{Zwicker04Perspective} and~\cite{Altman89QuaternionScandal}.

\section{Supervised learning }



\subsection{Unsupervised learning}

\subsection{Model refinement}
g. Carreria et al. [6] iteratively estimated error feedback from a shared weight model.
The output error feedback of the previous iteration is transformed into the input pose of the next iteration, which is
repeated several times for progressive pose refinement.
\subsection{Knowledge distillation}
Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. Within this framework enhancement of performance have been achieved in the field of classification \cite{DBLP:journals/corr/abs-1905-08094}